"""Streamlit Legal AI Helper

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vRTXuHbPDSG4z7AKIXU8gI7rAGOEu9Xx
"""

import streamlit as st
import json
import faiss
import numpy as np
import re
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any

# --- [CLASSES FROM YOUR COLAB NOTEBOOK] ---
# We copy the exact classes you defined in your notebook (Cells 10 & 12)

class LegalRAGSystem:
    """
    This class is copied from your original notebook (Cell 10).
    It handles the core retrieval and generation logic.
    """
    def __init__(self, index, chunks, metadata, embed_model, llm_model, k=5):
        self.index = index
        self.chunks = chunks
        self.metadata = metadata
        self.embed_model = embed_model
        self.llm_model = llm_model
        self.k = k

    def retrieve(self, query: str) -> List[Dict]:
        """Retrieve most relevant legal chunks for a query"""
        query_embedding = self.embed_model.encode([query])
        distances, indices = self.index.search(query_embedding.astype('float32'), self.k)

        results = []
        for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):
            if idx < len(self.chunks):  # Safety check
                results.append({
                    'chunk': self.chunks[idx],
                    'metadata': self.metadata[idx],
                    'score': float(dist),
                    'rank': i + 1
                })
        return results

    def format_context(self, retrieved_results: List[Dict]) -> str:
        """Format retrieved context for the LLM"""
        context_parts = []
        for i, result in enumerate(retrieved_results):
            source = result['metadata']['source']
            title = result['metadata']['title'][:100]  # Truncate long titles
            chunk_text = result['chunk']
            context_parts.append(f"[SOURCE {i+1}: {source} - {title}]\n{chunk_text}\n")
        return "\n".join(context_parts)

    def generate_answer(self, query: str, retrieved_results: List[Dict]) -> Dict:
        """Generate answer using Gemini with retrieved context"""
        context = self.format_context(retrieved_results)

        prompt = f"""You are an Indian Legal AI Assistant. Your role is to provide accurate, clear, and helpful answers about Indian law based ONLY on the provided legal context.

LEGAL CONTEXT:
{context}

USER QUESTION: {query}

INSTRUCTIONS:
1. Answer the question using ONLY the information from the legal context above.
2. If the context doesn't contain relevant information, say "I cannot find specific information about this in the provided legal sources."
3. Provide clear, plain-language explanations suitable for non-lawyers.
4. ALWAYS cite your sources by referring to the SOURCE numbers in brackets.
5. Be precise about legal provisions, sections, and case laws when mentioned in context.

ANSWER:"""

        try:
            response = self.llm_model.generate_content(prompt)
            answer = response.text
            return {
                'answer': answer,
                'sources': retrieved_results,
                'context_used': len(retrieved_results)
            }
        except Exception as e:
            st.error(f"Error generating answer from Gemini: {e}")
            return {
                'answer': "Error: Could not generate an answer.",
                'sources': retrieved_results,
                'context_used': len(retrieved_results)
            }

    def query(self, question: str) -> Dict:
        """Main query function - retrieve and generate"""
        retrieved = self.retrieve(question)
        result = self.generate_answer(question, retrieved)
        return result

class AdvancedLegalRAG(LegalRAGSystem):
    """
    This is your advanced class from Cell 12, inheriting from the base class.
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def calculate_confidence(self, retrieved_results: List[Dict]) -> float:
        """Calculate confidence score based on retrieval quality"""
        if not retrieved_results:
            return 0.0
        similarities = [1 / (1 + result['score']) for result in retrieved_results]
        avg_similarity = np.mean(similarities)
        confidence = min(avg_similarity * 2, 1.0) # Adjust scaling as needed
        return round(confidence, 2)

    def extract_citations(self, answer: str, retrieved_results: List[Dict]) -> List[Dict]:
        """Extract and format citations from answer"""
        citations = []
        for i, result in enumerate(retrieved_results):
            source_key = f"[SOURCE {i+1}]"
            # Check if the source key is in the answer
            if source_key in answer:
                citations.append({
                    'source_number': i + 1,
                    'document': result['metadata']['title'],
                    'source_type': result['metadata']['source'],
                    'content_preview': result['chunk'][:200] + "...",
                    'score': result['score']
                })
        return citations

    def query_with_analysis(self, question: str) -> Dict:
        """Enhanced query with confidence and citation analysis"""
        retrieved = self.retrieve(question)
        confidence = self.calculate_confidence(retrieved)
        result = self.generate_answer(question, retrieved)
        citations = self.extract_citations(result['answer'], retrieved)

        enhanced_result = {
            **result,
            'confidence': confidence,
            'citations': citations,
            'retrieval_metrics': {
                'total_chunks_retrieved': len(retrieved),
                'average_similarity_score': np.mean([1/(1+r['score']) for r in retrieved]) if retrieved else 0
            }
        }
        return enhanced_result

# --- [STREAMLIT APP LOGIC] ---

@st.cache_resource
def load_resources():
    """
    Load all the heavy models and data files ONCE.
    Streamlit's cache ensures this function only runs on the first load.
    """
    try:
        # 1. Load Embedding Model
        embed_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

        # 2. Configure Gemini LLM
        # --- IMPORTANT ---
        # In Streamlit, we use st.secrets instead of userdata.get()
        # You must add your GOOGLE_API_KEY to your Streamlit app's secrets.
        genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
        gemini_model = genai.GenerativeModel('gemini-2.5-flash')

        # 3. Load FAISS Index
        # This file MUST be in the same directory as app.py
        index = faiss.read_index('main_legal_index.faiss')

        # 4. Load Metadata and Chunks
        with open('main_chunk_metadata.json', 'r') as f:
            all_metadata = json.load(f)
        with open('main_chunks.json', 'r') as f:
            all_chunks = json.load(f)

        # 5. Initialize the RAG System
        rag_system = AdvancedLegalRAG(
            index=index,
            chunks=all_chunks,
            metadata=all_metadata,
            embed_model=embed_model,
            llm_model=gemini_model,
            k=5 # Retrieve top 5 chunks
        )
        return rag_system

    except FileNotFoundError:
        st.error(
            "ERROR: Critical files not found. "
            "Please make sure `main_legal_index.faiss`, `main_chunk_metadata.json`, "
            "and `main_chunks.json` are in the same folder as `app.py`."
        )
        return None
    except Exception as e:
        if "GOOGLE_API_KEY" in str(e):
             st.error(
                "ERROR: Google API Key not found. "
                "Please add your `GOOGLE_API_KEY` to your Streamlit app's secrets. "
                "Go to 'Deploy' > '...' > 'Settings' > 'Secrets'."
            )
        else:
            st.error(f"An error occurred during loading: {e}")
        return None

# --- [STREAMLIT UI] ---

st.set_page_config(page_title="Indian Legal AI Helper", layout="wide")

# Set title based on your project proposal
st.title("Legal AI Helper")
st.markdown("A context-aware AI assistant grounded in Indian law.")

# Load the RAG system (will be cached)
rag_system = load_resources()

if rag_system:
    # Get user query
    user_query = st.text_input(
        "Ask a question about Indian law:",
        placeholder="e.g., What is the punishment for theft under Indian law?"
    )

    if user_query:
        with st.spinner("Searching legal documents and generating an answer..."):
            try:
                # Run the full query analysis
                result = rag_system.query_with_analysis(user_query)

                # Display the answer
                st.markdown("### ðŸ¤– Answer")
                st.markdown(result['answer'])

                # Display confidence
                st.info(f"**Confidence:** {result['confidence'] * 100:.2f}%")

                # Display citations
                st.markdown("---")
                st.markdown("### ðŸ“š Citations Used")

                if result['citations']:

                    sorted_citations = sorted(result['citations'], key=lambda x: x['score'])
                    
                    for citation in result['citations']:
                        with st.expander(f"**Source {citation['source_number']}:** {citation['document']}"):
                            st.markdown(f"**Source Type:** {citation['source_type']}")
                            st.markdown(f"**Content Preview:**\n> {citation['content_preview']}")
                elif "[SOURCE" in result['answer']:
                     st.warning("The answer cited sources, but they could not be automatically extracted. Please review the retrieved sources below.")
                else:
                    st.markdown("No specific sources were cited in the answer.")

                # Optional: Show all retrieved sources for debugging/transparency
                with st.expander("See all retrieved documents (for reference)"):
                    st.json(result['sources'])

            except Exception as e:
                st.error(f"An error occurred while processing your query: {e}")
else:
    st.error("The Legal AI system could not be loaded. Please check the errors above.")
